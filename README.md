# ğŸ§  Brain-Age Prediction Challenge
The challenge is to predict chronological age of cognitively healthy adults using Graph Neural Networks (GNNs), applied to FreeSurfer-derived Structural MRI features from OASIS-3 dataset.

* **Task:** Regression
* **Target:** `age_at_visit` (in years)
* **Primary Metric:** Mean Absolute Error (MAE)

---

## ğŸ“Š Dataset Description
The dataset is curated from the **OASIS-3** longitudinal cohort. To ensure a pure aging baseline, we selected individuals who remained cognitively normal at all available visits (no MCI or dementia diagnoses).

* **Cohort:** 288 subjects (431 total sessions).
* **Preprocessing:** MRI scans were processed via the [FreeSurfer Pipeline](https://surfer.nmr.mgh.harvard.edu/). Processed data were sourced from the [Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC)](https://www.nitrc.org/projects/oasis3) repository. For simplicity, this challenge focuses exclusively on cortical measures (excluding subcortical and other regions). **Desikan-Killiany atlas** was used for cortical parcellation, which defines 68 cortical regions (34 per hemisphere).
* **Structural Features:** For each of the 68 cortical regions, two primary morphological metric is provided: **Cortical Thickness (mm)** and **Cortical Gray Matter Volume (mmÂ³)**.
* **Structural Connectomes:** Connectivity data (adjacency matrices) were sourced from [BrainGraph.org](https://braingraph.org/cms/), generated by the **PIT Bioinformatics Group**. The adjacency matrix for each 431 subject scans are provided. Steps to extract the adjacency matrix of each subject/sessions is described below.
---

## âš™ï¸ Graph Specifications
Each MRI session is represented as a graph:

* **Nodes ($N=68$):** Representing the 68 cortical regions.
* **Node Features:** A 2D vector per region:
    1.  **Cortical Thickness** (mm)
    2.  **Gray Matter Volume** (mmÂ³)
    * *Total Features:* 68 nodes $\times$ 2 metrics = 136 structural features per graph.
* **Edges:** Weighted by **dMRI Streamline Counts** (Fiber Counts) between regions. Adjacency matrices are provided for each session.
* **Target:** Scalar **Age at Visit** (in years)

---

## ğŸ“‚ Repository Structure

The dataset (431 total sessions) is split approximately into **80% Training**, **10% Validation**, and **10% Testing**. The split was performed at the **subject level** (ensuring no subject appears in more than one set) and was **stratified by age** to ensure a balanced distribution across all subsets.

* **`data/public/train_data.csv`**: 
  Contains data for 230 patients (353 sessions). Each row represents one MRI session.
  * **Subject**: Unique identifier for the participant.
  * **MR_session**: Unique identifier for the specific scan session.
  * **age_at_visit**: The target variable (chronological age in years).
  * **136 Cortical Measures**: Volume and thickness metrics for the 68 cortical regions.

* **`data/public/val_data.csv`**: 
  Contains 29 patients (39 sessions) for validation. Follows the same structure as the training set.

* **`data/public/test_data.csv`**: 
  Contains 29 patients (39 sessions). The target column (`age_at_visit`) is omitted.

* **`data/public/adjacency_matrices/`**: 
  A directory containing the 68x68 structural connectivity matrices for every session in the dataset.

---

## ğŸ“¥ Loading the Adjacency Matrices
The adjacency matrices are provided as CSV files under `data/public/adjacency_matrices/`. Each file is named by its Session ID (e.g., `OAS30001_d0757.csv`). You can load them as follows:

```python
import pandas as pd

# Load a specific session's adjacency matrix
file_path = 'data/public/adjacency_matrices/OAS30001_d0757.csv'
adj_df = pd.read_csv(file_path, index_col=0)

# The matrix is 68x68, where rows/cols match the order of regions in train_data.csv
print(adj_df.shape)  # Output: (68, 68)

```
---

## ğŸ“ Submission Instructions
To ensure your score is automatically calculated and added to the leaderboard, please follow these steps:

### 1. Folder Structure
Create a folder in `submissions/` named after your team:
`submissions/<your_team_name>/predictions.csv`

### 2. CSV Format
Your `predictions.csv` must contain exactly two columns:
| subject_session | age_at_visit |
| :--- | :--- |
| Test_Sub_001-Sess_01 | 70.5245 |
| Test_Sub_002-Sess_01 | 62.3873 |


### 3. Submission via Pull Request
1.  **Fork** this repository.
2.  Add your team folder and file.
3.  Submit a **Pull Request (PR)** to the `main` branch. 

## ğŸ† Leaderboard
Evaluation is performed automatically using **Mean Absolute Error (MAE)**. Once your PR is merged, the official rankings are updated.

ğŸ‘‰ **[View the Live Leaderboard Here](https://bjayadikary.github.io/brain-age-gnn-oasis3/leaderboard.html)**














# GNN Coding Competition Template

This repository provides a **secure, reproducible template** for running a
Graph Neural Network (GNN) competition that supports **humans and LLMs**
competing on equal footing.

The design intentionally **does not execute participant code**. Instead,
participants submit **predictions only**, which are automatically evaluated
and ranked on a public leaderboard using GitHub Actions.

This makes the competition:
- Safe (no untrusted code execution)
- Fully reproducible
- Suitable for human-vs-LLM evaluation studies

---

## 1. Task Overview

**Task:** Node classification on a graph  
**Input:** Public graph structure and node features  
**Output:** Predictions for unseen test nodes  
**Metric:** ROC-AUC (binary classification)

Participants train any GNN or non-GNN model *offline* and submit predictions
for the test nodes.

---

## 2. Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ train_edges.csv
â”‚   â”‚   â”œâ”€â”€ train_labels.csv
â”‚   â”‚   â”œâ”€â”€ val_edges.csv
â”‚   â”‚   â”œâ”€â”€ val_labels.csv
â”‚   â”‚   â”œâ”€â”€ test_edges.csv
â”‚   â”‚   â”œâ”€â”€ test_nodes.csv
â”‚   â”‚   â””â”€â”€ sample_submission.csv
â”‚   â””â”€â”€ private/
â”‚       â””â”€â”€ test_labels.csv   # never committed (used only in CI)
â”œâ”€â”€ competition/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ validate_submission.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ inbox/
â”œâ”€â”€ leaderboard/
â”‚   â”œâ”€â”€ leaderboard.csv
â”‚   â””â”€â”€ leaderboard.md
â””â”€â”€ .github/workflows/
    â”œâ”€â”€ score_submission.yml
    â””â”€â”€ publish_leaderboard.yml
```

---

## 3. Submission Format

Participants submit a **single CSV file**:

**predictions.csv**
```
id,y_pred
n0001,0.92
n0002,0.13
...
```

Rules:
- `id` must match exactly the IDs in `test_nodes.csv`
- One row per test node
- `y_pred` must be a float in [0,1]
- No missing or duplicate IDs

A sample is provided in:
```
data/public/sample_submission.csv
```

---

## 4. How to Submit
### 1. Prepare your Submission
Your submission must be a folder containing:
- `predictions.csv`: Your model's predictions.
- `metadata.json`: Information about your team and model.

**`predictions.csv` Format:**
The file must have exactly these two headers:
| subject_session | age_at_visit |
|:----------------|:-------------|
| Test_Sub_001-Sess_01 | 70.5 |
| Test_Sub_001-Sess_02 | 72.1 |

### 2. The Folder Structure
In your forked repository, place your files here:
`submissions/<your_team_name>/predictions.csv`

### 3. Submit via Pull Request
1. **Fork** this repository.
2. **Upload** your folder to the `submissions/` directory.
3. Open a **Pull Request (PR)** to the main repository.
4. Wait ~1 minute. A bot will automatically calculate your **MAE** and post it as a comment on your PR!
---

## 5. Leaderboard

After a PR is merged, the submission is added to:
- `leaderboard/leaderboard.csv`
- `leaderboard/leaderboard.md`

Rankings are sorted by **descending score**.

---

## 6. Rules

- No external or private data
- No manual labeling of test data
- No modification of evaluation scripts
- Unlimited offline training is allowed
- Only predictions are submitted

Violations may result in disqualification.

---

## 7. Human vs LLM Studies

To use this competition for research:
- Fix a time budget (e.g., 2 hours)
- Fix a submission budget (e.g., 5 runs)
- Record metadata fields (`model`, `llm_name`)
- Compare:
  - validity rate
  - best score within K submissions
  - score vs submission index

---

## 8. Citation

If you use this template in academic work, please cite the repository.

---

## 9. License

MIT License.

## Interactive Leaderboard (GitHub Pages)

This template includes an interactive leaderboard page inspired by modern benchmark sites.

**Enable GitHub Pages** (Settings â†’ Pages) and set the source to the `main` branch `/docs` folder.
Then open `https://<your-org>.github.io/<repo>/leaderboard.html`.
